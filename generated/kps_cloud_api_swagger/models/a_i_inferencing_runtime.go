// Code generated by go-swagger; DO NOT EDIT.

package models

// This file was generated by the swagger tool.
// Editing this file might prove futile when you re-run the swagger generate command

import (
	"encoding/json"

	strfmt "github.com/go-openapi/strfmt"

	"github.com/go-openapi/errors"
	"github.com/go-openapi/swag"
	"github.com/go-openapi/validate"
)

// AIInferencingRuntime AIInferencingRuntime defines the framework type and accelerator device.
// swagger:model AIInferencingRuntime
type AIInferencingRuntime struct {

	// accelerator device
	// Enum: [CPU GPU]
	AcceleratorDevice string `json:"AcceleratorDevice,omitempty"`

	// framework type
	// Enum: [TensorFlow1.13.1 TensorFlow2.1.0]
	FrameworkType string `json:"FrameworkType,omitempty"`
}

// Validate validates this a i inferencing runtime
func (m *AIInferencingRuntime) Validate(formats strfmt.Registry) error {
	var res []error

	if err := m.validateAcceleratorDevice(formats); err != nil {
		res = append(res, err)
	}

	if err := m.validateFrameworkType(formats); err != nil {
		res = append(res, err)
	}

	if len(res) > 0 {
		return errors.CompositeValidationError(res...)
	}
	return nil
}

var aIInferencingRuntimeTypeAcceleratorDevicePropEnum []interface{}

func init() {
	var res []string
	if err := json.Unmarshal([]byte(`["CPU","GPU"]`), &res); err != nil {
		panic(err)
	}
	for _, v := range res {
		aIInferencingRuntimeTypeAcceleratorDevicePropEnum = append(aIInferencingRuntimeTypeAcceleratorDevicePropEnum, v)
	}
}

const (

	// AIInferencingRuntimeAcceleratorDeviceCPU captures enum value "CPU"
	AIInferencingRuntimeAcceleratorDeviceCPU string = "CPU"

	// AIInferencingRuntimeAcceleratorDeviceGPU captures enum value "GPU"
	AIInferencingRuntimeAcceleratorDeviceGPU string = "GPU"
)

// prop value enum
func (m *AIInferencingRuntime) validateAcceleratorDeviceEnum(path, location string, value string) error {
	if err := validate.Enum(path, location, value, aIInferencingRuntimeTypeAcceleratorDevicePropEnum); err != nil {
		return err
	}
	return nil
}

func (m *AIInferencingRuntime) validateAcceleratorDevice(formats strfmt.Registry) error {

	if swag.IsZero(m.AcceleratorDevice) { // not required
		return nil
	}

	// value enum
	if err := m.validateAcceleratorDeviceEnum("AcceleratorDevice", "body", m.AcceleratorDevice); err != nil {
		return err
	}

	return nil
}

var aIInferencingRuntimeTypeFrameworkTypePropEnum []interface{}

func init() {
	var res []string
	if err := json.Unmarshal([]byte(`["TensorFlow1.13.1","TensorFlow2.1.0"]`), &res); err != nil {
		panic(err)
	}
	for _, v := range res {
		aIInferencingRuntimeTypeFrameworkTypePropEnum = append(aIInferencingRuntimeTypeFrameworkTypePropEnum, v)
	}
}

const (

	// AIInferencingRuntimeFrameworkTypeTensorFlow1131 captures enum value "TensorFlow1.13.1"
	AIInferencingRuntimeFrameworkTypeTensorFlow1131 string = "TensorFlow1.13.1"

	// AIInferencingRuntimeFrameworkTypeTensorFlow210 captures enum value "TensorFlow2.1.0"
	AIInferencingRuntimeFrameworkTypeTensorFlow210 string = "TensorFlow2.1.0"
)

// prop value enum
func (m *AIInferencingRuntime) validateFrameworkTypeEnum(path, location string, value string) error {
	if err := validate.Enum(path, location, value, aIInferencingRuntimeTypeFrameworkTypePropEnum); err != nil {
		return err
	}
	return nil
}

func (m *AIInferencingRuntime) validateFrameworkType(formats strfmt.Registry) error {

	if swag.IsZero(m.FrameworkType) { // not required
		return nil
	}

	// value enum
	if err := m.validateFrameworkTypeEnum("FrameworkType", "body", m.FrameworkType); err != nil {
		return err
	}

	return nil
}

// MarshalBinary interface implementation
func (m *AIInferencingRuntime) MarshalBinary() ([]byte, error) {
	if m == nil {
		return nil, nil
	}
	return swag.WriteJSON(m)
}

// UnmarshalBinary interface implementation
func (m *AIInferencingRuntime) UnmarshalBinary(b []byte) error {
	var res AIInferencingRuntime
	if err := swag.ReadJSON(b, &res); err != nil {
		return err
	}
	*m = res
	return nil
}
